run:
  task: 'TrainTask'
  runner: 'SequentialTrainer'
  seed: 2024

model:
  model_cls_name: 'MLPModelForMSELoss'
  config_cls_name: 'MSEMLPModelConfig'
  config:
    hidden_size: 256
    hidden_act: 'relu'
    classifier_dropout: 0.0
    hidden_dropout: 0.0
    num_mlp_layers: 0


embedding:
  cls_name: 'OpenAIEmbedding'
  config:
    name_or_path: 'text-embedding-3-large'
    max_length: 8192
    batch_size: 512
    use_cache: True
    cache_dir: '/mnt/elice/working/_embedding_cache'
    device: 'cpu'

collator:
  cls_name: 'CollatorForBinaryClassification'
  config:
    show_progress_bar: True
    precision: "float32"
    convert_to_numpy: False
    convert_to_tensor: True
    device: 'cpu'
    normalize_embeddings: True

dataset:
  cls_name: 'FinancialTranscriptBuilder'
  config:
    split:
      - 'train'
      - 'val'

trainer:
  output_dir: '/mnt/elice/working/outputs'
  run_name: 'OpenAI-8192-bs64-lr1.0e-5-linear' # for wandb
  learning_rate: &learning_rate 1.0e-5
  lr_scheduler_type: 'cosine'
  warmup_steps: &warmup_steps 1000
  weight_decay: &weight_decay 1.0e-1
  save_strategy: 'epoch'  # 에폭마다 모델을 저장
  logging_steps: 1000
  do_eval: True
  eval_strategy: 'epoch'  # 에폭마다 평가
  num_train_epochs: 50
  per_device_train_batch_size: &per_device_batch_size 64 # default 1024
  gradient_accumulation_steps: &gradient_accumulation 1
  # For memory efficiency.
  gradient_checkpointing: False
  fp16: &fp16 True # use torch.float16
  fp16_opt_level: '01'
  bf16: &bf16 False # Use torch.bfloat16
  group_by_length: False # Whether to order the sample by token length.
  use_cpu: False
  remove_unused_columns: False