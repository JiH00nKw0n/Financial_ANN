run:
  task: 'TrainTask'
  runner: 'SequentialTrainer'
  seed: 2024

model:
  model_cls_name: 'MLPModelForTokenClassification'
  config_cls_name: 'MLPModelConfig'
  config:
    hidden_size: 3072
    intermediate_size: 6144
    hidden_act: 'relu'
    classifier_dropout: 0.0
    hidden_dropout: 0.0
    num_labels: 2
    num_mlp_layers: 0


embedding:
  cls_name: 'OpenAIEmbedding'
  config:
    name_or_path: 'text-embedding-3-large'
    max_length: 8192
    batch_size: 512
    use_cache: True
    cache_dir: '/mnt/elice/working/_embedding_cache'
    device: 'cpu'

collator:
  cls_name: 'CollatorForBinaryClassification'
  config:
    show_progress_bar: True
    precision: "float32"
    convert_to_numpy: False
    convert_to_tensor: True
    device: 'cpu'
    normalize_embeddings: True

dataset:
  cls_name: 'FinancialTranscriptBuilder'
  config:
    split:
      - 'train'
      - 'val'

trainer:
  output_dir: '/mnt/elice/working/outputs'
  run_name : 'OpenAI-8192-bs64-lr1.0e-5-linear' # for wandb
  learning_rate: &learning_rate 1.0e-5
  lr_scheduler_type: 'cosine'
  warmup_steps: &warmup_steps 1280
  weight_decay: &weight_decay 1.0e-1
  save_steps: 1280
  logging_steps : 1
  do_eval: True
  eval_strategy: 'steps'
  eval_steps: 128
  num_train_epochs: 2
  per_device_train_batch_size : &per_device_batch_size 64 # default 1024
  gradient_accumulation_steps: &gradient_accumulation 1
  # For memory efficiency.
  gradient_checkpointing : False
  fp16: &fp16 True # use torch.float16
  fp16_opt_level : '01'
  bf16: &bf16 False # Use torch.bfloat16
  group_by_length : False # Whether to order the sample by token length.
  use_cpu : False
  remove_unused_columns : False