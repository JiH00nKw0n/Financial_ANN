run:
  task: 'LinqTrainTask'
  runner: 'RandomSamplerTrainer'
  seed: 2024

model:
  model_cls_name: 'LinqForSequenceClassification'
  config_cls_name: 'LinqConfig'
  config:
    torch_dtype: 'fp16'
  lora: 'configs/lora.yml'

collator:
  cls_name: 'CollatorForClassificationWithNeutral'
  config:
    max_length: 4096
  pretrained_model_name_or_path: 'Linq-AI-Research/Linq-Embed-Mistral'

dataset:
  cls_name: 'FinancialTranscriptBuilder'
  config:
    split: 'train'
    train_years:
      - 2010
      - 2011
      - 2012
      - 2013
      - 2014
      - 2015
      - 2016
    val_years:
      - 2017

trainer:
  output_dir: '/mnt/elice/working/outputs'
  run_name: 'Linq-4096-linear-bs128-lr-1.0e-4' # for wandb
  learning_rate: &learning_rate 1.0e-5
  lr_scheduler_type: 'cosine'
  warmup_steps: &warmup_steps 1000
  weight_decay: &weight_decay 1.0e-1
  save_strategy: 'epoch'  # 에폭마다 모델을 저장
  logging_steps: 1000
  do_eval: True
  eval_strategy: 'epoch'  # 에폭마다 평가
  num_train_epochs: 50
  per_device_train_batch_size: &per_device_batch_size 64 # default 1024
  gradient_accumulation_steps: &gradient_accumulation 1
  # For memory efficiency.
  gradient_checkpointing : True
  # NOTE : need to use this option to cope with DDP
  gradient_checkpointing_kwargs:
    use_reentrant: False
  ddp_find_unused_parameters: True
  fp16: &fp16 True # use torch.float16
  fp16_opt_level: '01'
  bf16: &bf16 False # Use torch.bfloat16
  group_by_length: False # Whether to order the sample by token length.
  use_cpu: False
  remove_unused_columns: False